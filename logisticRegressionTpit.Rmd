---
title: "logisticRegressionTpit"
author: "W. McDonald"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Logistic regression on Tpit data

Logistic regression will not tolerate missing values, so the same variable selection that I used in hierarchical clustering is used. 

```{r}
library(class)
library(tidyverse)

patma <- read_csv("data/TpitAddedDx")

df <- patma %>% 
  select(SF1Median, Pit1Median, TPITMedian, PRLMedian, GHMedian, TSHMedian, LHMedian, FSHMedian, ACTHMedian, ASUMedian, GATA3Median, newDx) %>%
  na.omit()

glimpse(df)
```

I need to convert my data into factors for this to work. Luckily, I already did this for Naive Bayes analysis. See the notes in that Rmd to consider whether additional levels should be introduced. In any case, use the following for now.

```{r}
df2  <-  df %>% 
  mutate(SF1Median = ifelse(SF1Median > 4, "Pos", "Neg"),
         Pit1Median = ifelse(Pit1Median > 4, "Pos", "Neg"),
         TPITMedian = ifelse(TPITMedian > 4, "Pos", "Neg"),
         PRLMedian = ifelse(PRLMedian > 4, "Pos", "Neg"),
         GHMedian = ifelse(GHMedian > 4, "Pos", "Neg"),
         TSHMedian = ifelse(TSHMedian > 4, "Pos", "Neg"),
         GATA3Median = ifelse(GATA3Median > 4, "Pos", "Neg"),
         LHMedian = ifelse(LHMedian > 4, "Pos", "Neg"),
         FSHMedian = ifelse(FSHMedian > 4, "Pos", "Neg"),
         ACTHMedian = ifelse(ACTHMedian > 4, "Pos", "Neg"),
         ASUMedian = ifelse(ASUMedian > 4, "Pos", "Neg")
         ) 

glimpse(df2)
```

Oops. These are still in character class, NOT factors. Some advice from StackOverflow: <https://stackoverflow.com/questions/9251326/convert-data-frame-column-format-from-character-to-factor>.

```{r}
df2[] <- lapply(df2, factor) # the "[]" keeps the dataframe structure
 col_names <- names(df2)
# do it for some names in a vector named 'col_names'
df2[col_names] <- lapply(df2[col_names] , factor)

str(df2)
```

I'd like to focus on stepwise regression, since this helps with something that I haven't worked with much before: *feature selection*. 

```{r}
null_model <- glm(newDx ~ 1, data = df2, family = "binomial")
full_model <- glm(newDx ~ ., data = df2, family = "binomial")
step_model <- step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")
step_prob <- predict(step_model, type = "response")

summary(step_prob)
summary(step_model)

library(pROC)

ROC <- roc(df2$newDx, step_prob)
plot(ROC, col = "red")
auc(ROC)
```

Right, well that isn't working out the way that I'd hoped. 

Maybe just try a simpler problem.

```{r}
model <- glm(newDx ~ SF1Median + Pit1Median + TPITMedian, 
                      data = df2, family = "binomial")

# Summarize the model results
summary(model)

prob <- predict(model, type = "response")

library(pROC)

ROC <- roc(df2$newDx, prob)
plot(ROC, col = "red")
auc(ROC)
```

Return to this after finDx established and rerun. Consider using ifelse() to change finDx to dichotomous classes (for example GON and nonGON), then rerun. 

Also, consider <https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression> conversations about the warning messages "1: glm.fit: fitted probabilities numerically 0 or 1 occurred." 


```{r}
knitr::knit_exit()
```

This is scrap work.